import atexit
import streamlit as st
import pandas as pd
from datetime import datetime
import pytz
from tzlocal import get_localzone
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from pathlib import Path
from croniter import croniter
from io import StringIO
from utils.data import GPTdata
from utils.gpt_request import process_image_resource, process_text_resource, history_data

def get_image_data():
    history_data()
    select_sql = """SELECT
    t1.region AS 'åŒºåŸŸ',
    CASE
        WHEN t1.model_name like 'gpt-4o%' THEN t1.deployment_name || ' ( ' || t1.model_version || ' )'
        ELSE t1.model_name || ' ( ' || t1.model_version || ' )'
    END AS 'æ¨¡å‹',
    MAX(t1.update_time) AS 'æ›´æ–°æ—¶é—´ï¼ˆ+8ï¼‰',
    t1.status AS 'çŠ¶æ€',
    CAST(ROUND(AVG(t1.input_tokens), 0) AS INTEGER) || ' / ' || t1.input_content_length AS 'è¾“å…¥Tokens/é•¿åº¦',
    CAST(ROUND(AVG(t1.output_tokens), 0) AS INTEGER) || ' / ' || t1.output_content_length AS 'è¾“å‡ºTokens/é•¿åº¦',
    ROUND(AVG(t1.total_time), 4) AS 'æ€»æ—¶é—´',
    t2.totalTime as 'æ€»æ—¶é—´(å†å²è§†å›¾)',
    ROUND(AVG(t1.starttransfer_time), 4) AS 'é¦–Tokenæ—¶é—´',
    t2.starttransferTime as 'é¦–Tokenæ—¶é—´(å†å²è§†å›¾)'
FROM
    gpt_latency_data AS t1
JOIN
    (
        SELECT
            region,
            deployment_type,
            deployment_name,
            model_name,
            model_version,
            type,
            '[' || GROUP_CONCAT(avg_total_time) || ']' AS totalTime,
            '[' || GROUP_CONCAT(avg_starttransfer_time) || ']' AS starttransferTime
        FROM
            (
                SELECT
                    region,
                    deployment_type,
                    deployment_name,
                    model_name,
                    model_version,
                    type,
                    strftime('%Y%m%d', create_time) AS createTime,
                    ROUND(AVG(total_time), 4) AS avg_total_time,
                    ROUND(AVG(starttransfer_time), 4) AS avg_starttransfer_time
                FROM
                    gpt_latency_data_history
                WHERE
                    type = 'IMAGE'
                GROUP BY
                    region,
                    deployment_type,
                    deployment_name,
                    model_name,
                    model_version,
                    type,
                    createTime
            ) AS subquery
        GROUP BY
            region,
            deployment_type,
            deployment_name,
            model_name,
            model_version,
            type
    ) AS t2
    ON t1.region = t2.region
    AND t1.deployment_type = t2.deployment_type
    AND t1.deployment_name = t2.deployment_name
    AND t1.model_name = t2.model_name
    AND t1.model_version = t2.model_version
    AND t1.type = t2.type
WHERE
    t1.type = 'IMAGE'
GROUP BY
    t1.region,
    t1.deployment_type,
    t1.deployment_name,
    t1.model_name,
    t1.model_version,
    t1.type
ORDER BY
    t1.deployment_name;"""
    db = GPTdata()
    data = db.query(select_sql)
    return data

def get_text_data():
    history_data()
    select_sql = """SELECT
        t1.region as 'åŒºåŸŸ',
        CASE
          WHEN t1.model_name like 'gpt-4o%' THEN t1.deployment_name || ' ( ' ||  t1.model_version || ' )'
          ELSE t1.model_name || ' ( ' || t1.model_version || ' )'
        END as 'æ¨¡å‹',
        MAX(t1.update_time) AS 'æ›´æ–°æ—¶é—´ï¼ˆ+8ï¼‰',
        t1.status as 'çŠ¶æ€',
        CAST(ROUND(AVG(t1.input_tokens), 0) AS INTEGER) || ' / ' || t1.input_content_length AS 'è¾“å…¥Tokens/é•¿åº¦',
        CAST(ROUND(AVG(t1.output_tokens), 0) AS INTEGER) || ' / ' || t1.output_content_length AS 'è¾“å‡ºTokens/é•¿åº¦',
        ROUND(AVG(t1.total_time), 4) AS 'æ€»æ—¶é—´',
        t2.totalTime as 'æ€»æ—¶é—´(å†å²è§†å›¾)',
        ROUND(AVG(t1.starttransfer_time), 4) AS 'é¦–Tokenæ—¶é—´',
        t2.starttransferTime as 'é¦–Tokenæ—¶é—´(å†å²è§†å›¾)'
FROM
        gpt_latency_data AS t1
        JOIN
        (
            SELECT
                region,
                deployment_type,
                deployment_name,
                model_name,
                model_version,
                type,
                '[' || GROUP_CONCAT(avg_total_time) || ']' AS totalTime,
                '[' || GROUP_CONCAT(avg_starttransfer_time) || ']' AS starttransferTime
            FROM (
                SELECT
                    region,
                    deployment_type,
                    deployment_name,
                    model_name,
                    model_version,
                    type,
                    strftime('%Y%m%d', create_time) AS createTime,
                    ROUND(AVG(total_time), 4) AS avg_total_time,
                    ROUND(AVG(starttransfer_time), 4) AS avg_starttransfer_time
                FROM
                    gpt_latency_data_history
                WHERE
                    type = 'TEXT'
                GROUP BY
                    region,
                    deployment_type,
                    deployment_name,
                    model_name,
                    model_version,
                    type,
                    createTime
            ) AS subquery
            GROUP BY
                region,
                deployment_type,
                deployment_name,
                model_name,
                model_version,
                type
        ) AS t2
        ON t1.region = t2.region
        AND t1.deployment_type = t2.deployment_type
        AND t1.deployment_name = t2.deployment_name
        AND t1.model_name = t2.model_name
        AND t1.model_version = t2.model_version
        AND t1.type = t2.type
WHERE t1.type = 'TEXT'
GROUP BY
        t1.region,
        t1.deployment_type,
        t1.deployment_name,
        t1.model_name,
        t1.model_version,
        t1.type
ORDER BY t1.deployment_name;
    """
    db = GPTdata()
    data = db.query(select_sql)
    return data

def convert_to_utc_plus_8(utc_time_str):
    time_format = "%Y-%m-%d %H:%M:%S"
    utc_time = datetime.strptime(utc_time_str, time_format)
    utc_time = utc_time.replace(tzinfo=pytz.utc)
    utc_plus_8_time = utc_time.astimezone(pytz.timezone("Asia/Shanghai"))
    return utc_plus_8_time.strftime(time_format)

st.set_page_config(
    page_title="Azure GPT Latency",  # ç½‘é¡µæ ‡é¢˜
    page_icon="ğŸª",                         # ç½‘é¡µå›¾æ ‡ï¼Œå¯ä»¥æ˜¯ emoji
    layout="wide"                           # ç½‘é¡µå¸ƒå±€ï¼Œä½¿ç”¨ "wide" ä½¿å†…å®¹å æ®æ•´ä¸ªå®½åº¦
)

st.title("Azure GPT å„åŒºæ€§èƒ½ (ä»…ä¾›å‚è€ƒ)")
# æœ¬åœ°æ—¶åŒº
local_timezone = get_localzone()
local_datetime = datetime.now(local_timezone)
local_time_str = local_datetime.strftime("%Y-%m-%d %H:%M:%S")
st.write(f'å®¢æˆ·ç«¯æ—¶é—´: {local_datetime.strftime("%Y-%m-%d %H:%M:%S %Z%z")} ( {local_timezone} ), åŒ—äº¬æ—¶é—´ï¼š{convert_to_utc_plus_8(local_time_str)} ( Asia/Shanghai )')
# st.write("æ¯ä¸¤æ—¶æ‰§è¡Œä¸€æ¬¡, å‘èµ·ç«¯åœ¨éŸ©å›½ï¼Œ")
with st.expander(f"æ¯ä¸¤æ—¶æ‰§è¡Œä¸€æ¬¡, å‘èµ·ç«¯åœ¨éŸ©å›½ï¼Œ æŸ¥çœ‹å…¶ä»–è¯´æ˜"):
    st.text("""
    1. æ¯ä¸¤å°æ—¶å‘èµ·ä¸€æ¬¡å“åº”æ—¶é—´æµ‹è¯•ï¼Œæ¯æ¬¡è¯·æ±‚è¿”å›é™åˆ¶åœ¨60ä¸ªtokensï¼Œæ¯æ¬¡è¿ç»­å‘é€ä¸‰ä¸ªè¯·æ±‚ï¼Œç»“æœå–å¹³å‡å€¼
    2. ä¿ç•™æœ€è¿‘10å¤©çš„å†å²æµ‹è¯•ç»“æœï¼Œå–æ¯å¤©æ•°æ®çš„å¹³å‡å€¼ã€‚
    3. çŠ¶æ€ï¼šâœ…è¡¨ç¤ºæ­£å¸¸ï¼ŒâŒè¡¨ç¤ºå¼‚å¸¸ï¼ˆå¯èƒ½è®¤è¯å¤±è´¥/è¶…æ—¶/è§¦å‘æ¥å£é™åˆ¶ç­‰ï¼‰ï¼›
""")
# st.markdown("---")
st.subheader("å›¾ç‰‡ä»»åŠ¡ï¼š")
img_data = get_image_data()
if img_data is None or len(img_data) == 0:
    st.write("æš‚æ— æ•°æ®")
else:
    img_df = pd.DataFrame(img_data)

    # è®¡ç®—è¡Œæ•°å¹¶è®¾ç½®é«˜åº¦
    row_height = 35  # æ¯è¡Œçš„é«˜åº¦ï¼ˆåƒç´ ï¼‰
    header_height = 100  # è¡¨å¤´çš„é«˜åº¦ï¼ˆåƒç´ ï¼‰
    num_rows = img_df.shape[0]
    total_height = header_height + (len(img_df) * row_height) + row_height

    # æ³¨å…¥è‡ªå®šä¹‰ CSS æ ·å¼
    st.markdown(
        f"""
        <style>
        .dataframe {{
            height: {total_height}px !important;
            overflow: hidden !important;
        }}
        </style>
        """,
        unsafe_allow_html=True
    )
    # st.table(df)
    height = len(img_df) * 38
    img_max = img_df["æ€»æ—¶é—´"].max()
    img_df["æ›´æ–°æ—¶é—´ï¼ˆ+8ï¼‰"] = img_df["æ›´æ–°æ—¶é—´ï¼ˆ+8ï¼‰"].apply(convert_to_utc_plus_8)
    img_df['çŠ¶æ€'] = img_df['çŠ¶æ€'].apply(lambda x: f'âœ…{x}' if x == 200 else f'âŒ{x}')
    # æ˜¾ç¤ºæ•°æ®è¡¨æ ¼
    # st.dataframe(styled_df, hide_index=True)
    st.dataframe(
        img_df,
        column_config={
            "æ€»æ—¶é—´": st.column_config.ProgressColumn(
                "æ€»æ—¶é—´",
                help="å–ä¸‰æ¬¡è¯·æ±‚çš„å¹³å‡å€¼",
                format="%f",
                min_value=0,
                max_value=img_max,
            ),
            "æ€»æ—¶é—´(å†å²è§†å›¾)": st.column_config.LineChartColumn(
                "æ€»æ—¶é—´(å†å²è§†å›¾)",
                width="medium",
                help="10å¤©å†…çš„å†å²æ•°æ®",
            ),
            "é¦–Tokenæ—¶é—´": st.column_config.ProgressColumn(
                "é¦–Tokenæ—¶é—´",
                help="å–ä¸‰æ¬¡è¯·æ±‚çš„å¹³å‡å€¼",
                format="%f",
                min_value=0,
                max_value=img_max,
            ),
            "é¦–Tokenæ—¶é—´(å†å²è§†å›¾)": st.column_config.LineChartColumn(
                "é¦–Tokenæ—¶é—´(å†å²è§†å›¾)",
                width="medium",
                help="10å¤©å†…çš„å†å²æ•°æ®",
            ),
        },
        height=height,
        hide_index=True,
    )
st.markdown("---")
st.subheader("æ–‡æœ¬ä»»åŠ¡ï¼š")
text_data = get_text_data()
if text_data is None or len(text_data) == 0:
    st.write("æš‚æ— æ•°æ®")
else:
    text_df = pd.DataFrame(text_data)
    # st.table(df)
    text_height = len(text_df) * 37
    text_max = text_df["æ€»æ—¶é—´"].max()
    text_df["æ›´æ–°æ—¶é—´ï¼ˆ+8ï¼‰"] = text_df["æ›´æ–°æ—¶é—´ï¼ˆ+8ï¼‰"].apply(convert_to_utc_plus_8)
    text_df['çŠ¶æ€'] = text_df['çŠ¶æ€'].apply(lambda x: f'âœ…{x}' if x == 200 else f'âŒ{x}')
    # æ˜¾ç¤ºæ•°æ®è¡¨æ ¼
    # st.dataframe(styled_df, hide_index=True)
    st.dataframe(
        text_df,
        column_config={
            "æ€»æ—¶é—´": st.column_config.ProgressColumn(
                "æ€»æ—¶é—´",
                help="å–ä¸‰æ¬¡è¯·æ±‚çš„å¹³å‡å€¼",
                format="%f",
                min_value=0,
                max_value=text_max,
            ),
            "æ€»æ—¶é—´(å†å²è§†å›¾)": st.column_config.LineChartColumn(
                "æ€»æ—¶é—´(å†å²è§†å›¾)",
                width="medium",
                help="10å¤©å†…çš„å†å²æ•°æ®",
            ),
            "é¦–Tokenæ—¶é—´": st.column_config.ProgressColumn(
                "é¦–Tokenæ—¶é—´",
                help="å–ä¸‰æ¬¡è¯·æ±‚çš„å¹³å‡å€¼",
                format="%f",
                min_value=0,
                max_value=text_max,
            ),
            "é¦–Tokenæ—¶é—´(å†å²è§†å›¾)": st.column_config.LineChartColumn(
                "é¦–Tokenæ—¶é—´(å†å²è§†å›¾)",
                width="medium",
                help="10å¤©å†…çš„å†å²æ•°æ®",
            ),
        },
        height=text_height,
        hide_index=True,
    )

st.markdown("---")
st.write("Support by Min")
st.write("Reference https://github.com/lianyutianshi77/AzureGPTmonitorWithHistory")


# st.markdown("---")
# st.markdown("#### æ•°æ®æŸ¥è¯¢ï¼š")
# query = st.text_area('æ•°æ®æŸ¥è¯¢ï¼š', value="""select count(*) as æ•°é‡ from gpt_latency_data_history;""", help='è¾“å…¥SQLæŸ¥è¯¢è¯­å¥, ä¸æ”¯æŒéSelectä»¥å¤–çš„æ“ä½œ', label_visibility="collapsed")
# if st.button('æ‰§è¡ŒæŸ¥è¯¢'):
#     if query and query.strip().lower().startswith('select'):
#         try:
#             azure_data = GPTdata()
#             df = None
#             with st.spinner('æŸ¥è¯¢ä¸­ï¼Œè¯·ç¨å€™...'):
#                 progress_bar = st.progress(0)
#                 df = azure_data.query(query)

#             # æ£€æŸ¥æŸ¥è¯¢ç»“æœå¹¶æ˜¾ç¤ºç›¸åº”ä¿¡æ¯
#             if df:
#                 st.write(f"æŸ¥è¯¢åˆ°{len(df)}æ¡æ•°æ® . . . ")
#                 st.dataframe(df)
#                 # st.write(df)
#             else:
#                 st.error("æŸ¥è¯¢ç»“æœä¸ºç©ºï¼Œè¯·æ£€æŸ¥æŸ¥è¯¢æ¡ä»¶ï¼")
#         except Exception as e:
#             st.error("æŸ¥è¯¢å¤±è´¥")
#     else:
#         st.error("ä¸æ”¯æŒéSelectä»¥å¤–çš„æ“ä½œï¼Œè¯·æ£€æŸ¥ï¼")

# åˆå§‹åŒ–è°ƒåº¦å™¨
scheduler = BackgroundScheduler()
cron_expression = "25 */2 * * *"  # crontab è¡¨è¾¾å¼ï¼Œ æ¯ä¸¤å°æ—¶æ‰§è¡Œä¸€æ¬¡
scheduler_lock = "scheduler.lock"

def update_next_time():
    cron = croniter(cron_expression, datetime.now())
    next_time = cron.get_next(datetime).strftime("%Y-%m-%d %H:%M:%S")
    st.session_state['next_time'] = next_time
    print(f"ä¸‹æ¬¡ä»»åŠ¡æ‰§è¡Œæ—¶é—´: {next_time}")

    if Path(scheduler_lock).exists():
        df = pd.read_json(scheduler_lock, orient='records')
    else:
        df = pd.DataFrame(columns=["job", "next_time"])

    df["next_time"] = next_time
    df.to_json(scheduler_lock, orient='records')

def run_task():
    print(f"å®šæ—¶ä»»åŠ¡è¿è¡Œä¸­... {datetime.now()}")
    process_image_resource()
    process_text_resource()
    print(f"å®šæ—¶ä»»åŠ¡ç»“æŸ... {datetime.now()}")
    update_next_time()

def start_scheduler():
    if 'job' not in st.session_state:
        try:
            update_next_time()
            trigger = CronTrigger.from_crontab(cron_expression)
            job = scheduler.add_job(run_task, trigger, max_instances=20)
            st.session_state['job'] = job

            df = pd.DataFrame([{"job":  str(st.session_state['job']), "next_time": st.session_state['next_time']}])
            df.to_json(scheduler_lock, orient='records')

            scheduler.start()
            print(f"ä»»åŠ¡å¯åŠ¨æˆåŠŸ, å¾…æ‰§è¡Œæ—¶é—´: {st.session_state['next_time']}")
        except Exception as e:
            print(f"å¯åŠ¨ä»»åŠ¡æ—¶å‡ºé”™: {e}")
    else:
        print(f"ä»»åŠ¡ä¿¡æ¯ï¼š{st.session_state['job']}")
        print("å·²æœ‰ä»»åŠ¡åœ¨è¿è¡Œä¸­!")

def load_existing_scheduler():
    if Path(scheduler_lock).exists():
        df = pd.read_json(scheduler_lock, orient='records')
        print(f"å·²æœ‰è°ƒåº¦å™¨åœ¨è¿è¡Œä¸­! {df.to_dict()}")
    else:
        start_scheduler()

def cleanup():
    print("åº”ç”¨é€€å‡ºï¼Œæ¸…ç†èµ„æº...")
    if scheduler.running:
        scheduler.shutdown()

atexit.register(cleanup)

# åˆå§‹åŒ–
load_existing_scheduler()
